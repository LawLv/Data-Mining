PNN
有效捕捉高阶特征交互的能力

嵌入层：
将高维稀疏输入数据（例如，通过独热编码转换的分类数据）转换为稠密的低维向量。
这一层有助于处理维度诅咒并减少计算复杂度。
乘积层：
基于内积的神经网络（IPNN）：使用内积操作来建模特征交互。内积通过取点积计算不同特征嵌入之间的交互，捕捉它们的组合效果。
基于外积的神经网络（OPNN）：利用外积操作捕捉特征交互。外积创建一个矩阵，表示特征嵌入之间所有可能的交互，提供更丰富的表示。
全连接层（MLP）：
在乘积层之后，使用多个全连接层来学习更高阶的特征模式。这些层进一步提炼和转换组合的特征交互，生成有意义的预测。
常用ReLU（修正线性单元）作为这些层的激活函数，因为其效率和性能优异。
输出层：
最后一层使用Sigmoid激活函数生成预测概率（例如，点击的可能性），输出介于0和1之间的值。

PNN的特征
通过使用乘积操作（内积或外积），PNN可以捕捉特征之间复杂的关系，而传统的加性模型（如线性模型或简单的MLP）可能会错过这些关系。
高性能：
PNN在用户响应预测任务中一致优于最先进的模型，展示了其在捕捉数据复杂交互方面的强大能力。
https://github.com/search?q=Product-based%20neural%20networks%20for%20user%20response%20prediction&type=repositories





Wide & Deep Learning模型的原理和特点
将线性模型（Wide部分）和深度神经网络（Deep部分）联合训练，从而兼顾记忆和泛化的优势，减少过拟合

Wide部分：
是一个广义线性模型（如逻辑回归），用于捕捉特征之间的交互关系。通过对特征进行交叉乘积变换，可以有效地记忆历史数据中频繁出现的特征组合。
优点是简单、可解释且容易扩展，但缺点是对于新的或稀疏的数据组合无法很好地泛化，需要大量的特征工程。
Deep部分：
是一个深度神经网络，用于从稀疏特征中学习低维的密集嵌入表示。这些嵌入表示能更好地捕捉稀疏特征的潜在关系，从而提高模型的泛化能力。
优点是能够自动学习特征表示并对未见过的特征组合进行预测，但在数据稀疏且高秩的情况下可能会过度泛化，导致推荐不准确。

Wide & Deep集成：
通过联合训练，Wide部分记忆历史上频繁出现的特征组合，Deep部分学习稀疏特征的低维表示，从而结合两者的优势。
在训练过程中，通过对Wide和Deep部分的输出进行加权求和并共同使用一个逻辑损失函数进行优化。这样做能够在训练时同时优化两个部分的参数，实现更好的性能。
这种联合训练不同于简单的模型集成（ensemble），它能够更高效地利用参数，减少模型的复杂性和训练时间。







https://github.com/batch-norm/xDeepFM
结合了深度神经网络（DNN）和压缩交互网络（CIN），旨在显式和矢量级别生成特征交互
xDeepFM的优势
xDeepFM通过以下方式解决了上述问题：

显式特征交互生成：xDeepFM使用压缩交互网络（CIN）在矢量级别上生成显式特征交互。
结合低阶和高阶特征交互：通过将CIN与经典DNN结合，xDeepFM能够显式学习一定程度的特征交互，并隐式学习任意低阶和高阶特征交互。
通用性和性能：综合实验结果表明，xDeepFM在三个真实世界数据集上均优于当前最先进的模型

压缩交互网络（CIN）
CIN的设计灵感来自卷积神经网络（CNN）和循环神经网络（RNN），它在显式和矢量级别生成特征交互，能够更好地捕捉和建模特征之间的关系。CIN通过压缩特征交互的方式减少计算量，同时保留了重要的特征信息。

模型结构
xDeepFM模型结构包括：

Embedding层：将稀疏的高维特征向量转换为低维稠密向量。
CIN层：显式生成特征交互。
DNN层：隐式学习高阶特征交互。
输出层：综合前面的特征交互，进行最终的点击率预测。




